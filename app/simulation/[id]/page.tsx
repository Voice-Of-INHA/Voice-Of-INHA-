"use client";

import { useEffect, useState, useRef, useCallback } from "react";
import { useRouter, useParams } from "next/navigation";

// ì¸í„°í˜ì´ìŠ¤ ì •ì˜
interface Round {
  round: number;
  question: string;
  audio_url: string;
}

interface Scenario {
  id: number;
  title: string;
  rounds: Round[];
  guideline: string;
}

interface UserResponse {
  round: number;
  audioBlob: Blob;
  transcription?: string;
}

interface TranscriptResponse {
  round: number;
  transcript: string;
}

interface AllResponsesData {
  success: boolean;
  responses: TranscriptResponse[];
}

interface AnalysisResult {
  analysis: string;
}

interface WavRecording {
  audioData: number[];
  processor: ScriptProcessorNode;
  source: MediaStreamAudioSourceNode;
  sampleRate: number;
}

// WebKit AudioContext íƒ€ì… ì •ì˜(ì‚¬íŒŒë¦¬ ëŒ€ì‘)
declare global {
  interface Window {
    webkitAudioContext?: typeof AudioContext;
    currentWavRecording?: WavRecording | null;
  }
}

// ê°œì„ ëœ VAD (Voice Activity Detection) ëª¨ë“ˆ
const VoiceActivityDetector = {
  // ì¡°ì •ëœ ì„ê³„ê°’ë“¤
  VOLUME_THRESHOLD: 15, // ë” ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ ë¯¼ê°ë„ ì¦ê°€
  SILENCE_DURATION: 1500, // 1.5ì´ˆë¡œ ë‹¨ì¶•
  MIN_SPEECH_DURATION: 500, // ìµœì†Œ ë°œí™” ì‹œê°„ (0.5ì´ˆ)
  
  analyser: null as AnalyserNode | null,
  dataArray: null as Uint8Array | null,
  isInitialized: false,
  
  // ìŒì„± ìƒíƒœ ì¶”ì 
  lastVoiceTime: 0,
  speechStartTime: 0,
  isSpeaking: false,
  volumeHistory: [] as number[],
  
  init(audioContext: AudioContext, source: MediaStreamAudioSourceNode) {
    try {
      this.analyser = audioContext.createAnalyser();
      this.analyser.fftSize = 512; // ë” ì„¸ë°€í•œ ë¶„ì„ì„ ìœ„í•´ ì¦ê°€
      this.analyser.smoothingTimeConstant = 0.8; // ë” ë¶€ë“œëŸ¬ìš´ í‰í™œí™”
      this.analyser.minDecibels = -90;
      this.analyser.maxDecibels = -10;
      
      source.connect(this.analyser);
      const bufferLength = this.analyser.frequencyBinCount;
      this.dataArray = new Uint8Array(bufferLength);
      this.isInitialized = true;
      
      // ìƒíƒœ ì´ˆê¸°í™”
      this.lastVoiceTime = 0;
      this.speechStartTime = 0;
      this.isSpeaking = false;
      this.volumeHistory = [];
      
      console.log("ê°œì„ ëœ VAD ì´ˆê¸°í™” ì™„ë£Œ:", { 
        bufferLength, 
        fftSize: this.analyser.fftSize,
        smoothingTimeConstant: this.analyser.smoothingTimeConstant
      });
    } catch (error) {
      console.error("VAD ì´ˆê¸°í™” ì‹¤íŒ¨:", error);
      this.isInitialized = false;
    }
  },

  getVolume(): number {
    if (!this.isInitialized || !this.analyser || !this.dataArray) return 0;
    
    try {
      
      // ë” ì •êµí•œ ë³¼ë¥¨ ê³„ì‚° (RMS ë°©ì‹)
      let sum = 0;
      let count = 0;
      
      // ì¤‘ê°„ ì£¼íŒŒìˆ˜ ëŒ€ì—­ì— ì§‘ì¤‘ (ì¸ê°„ ìŒì„± ì£¼íŒŒìˆ˜)
      const start = Math.floor(this.dataArray.length * 0.1);
      const end = Math.floor(this.dataArray.length * 0.8);
      
      for (let i = start; i < end; i++) {
        const value = this.dataArray[i];
        sum += value * value;
        count++;
      }
      
      const rms = Math.sqrt(sum / count);
      
      // ë³¼ë¥¨ íˆìŠ¤í† ë¦¬ ê´€ë¦¬ (ìµœê·¼ 10ê°œ ê°’)
      this.volumeHistory.push(rms);
      if (this.volumeHistory.length > 10) {
        this.volumeHistory.shift();
      }
      
      // í‰ê·  ë³¼ë¥¨ ê³„ì‚°
      const avgVolume = this.volumeHistory.reduce((a, b) => a + b, 0) / this.volumeHistory.length;
      
      return avgVolume;
    } catch (e) {
      console.error("ë³¼ë¥¨ ì¸¡ì • ì˜¤ë¥˜:", e);
      return 0;
    }
  },

  isVoiceDetected(): boolean {
    const currentTime = Date.now();
    const volume = this.getVolume();
    const isVoice = volume > this.VOLUME_THRESHOLD;
    
    if (isVoice) {
      this.lastVoiceTime = currentTime;
      
      if (!this.isSpeaking) {
        this.speechStartTime = currentTime;
        this.isSpeaking = true;
        console.log("ğŸ¤ ìŒì„± ê°ì§€ ì‹œì‘!", { volume, threshold: this.VOLUME_THRESHOLD });
      }
    }
    
    // ìŒì„±ì´ ê°ì§€ë˜ì§€ ì•Šê³  ì¼ì • ì‹œê°„ì´ ì§€ë‚¬ë‹¤ë©´
    if (!isVoice && this.isSpeaking) {
      const silenceDuration = currentTime - this.lastVoiceTime;
      const speechDuration = this.lastVoiceTime - this.speechStartTime;
      
      if (silenceDuration > this.SILENCE_DURATION && speechDuration > this.MIN_SPEECH_DURATION) {
        this.isSpeaking = false;
        console.log("ğŸ”‡ ìŒì„± ì¢…ë£Œ ê°ì§€!", { 
          silenceDuration, 
          speechDuration,
          minSpeechDuration: this.MIN_SPEECH_DURATION,
          silenceThreshold: this.SILENCE_DURATION
        });
        return false;
      }
    }
    
    return this.isSpeaking;
  },

  // ìŒì„± ì¢…ë£Œ ì²´í¬ (ë³„ë„ ë©”ì„œë“œ)
  checkSpeechEnd(): boolean {
    if (!this.isSpeaking) return false;
    
    const currentTime = Date.now();
    const silenceDuration = currentTime - this.lastVoiceTime;
    const speechDuration = this.lastVoiceTime - this.speechStartTime;
    
    const shouldEnd = silenceDuration > this.SILENCE_DURATION && speechDuration > this.MIN_SPEECH_DURATION;
    
    if (shouldEnd) {
      this.isSpeaking = false;
      console.log("âœ… ìŒì„± ì¢…ë£Œ í™•ì •!", { 
        silenceDuration, 
        speechDuration,
        totalSpeechTime: speechDuration
      });
    }
    
    return shouldEnd;
  },

  reset() {
    this.lastVoiceTime = 0;
    this.speechStartTime = 0;
    this.isSpeaking = false;
    this.volumeHistory = [];
    console.log("VAD ìƒíƒœ ë¦¬ì…‹");
  },

  cleanup() {
    this.analyser = null;
    this.dataArray = null;
    this.isInitialized = false;
    this.reset();
  },
};

export default function SimulationPage() {
  const router = useRouter();
  const params = useParams();
  const scenarioId = params.id as string;

  // ë”ë¯¸ ë°ì´í„° ì‚¬ìš©
  const dummyScenario: Scenario = {
    id: 1,
    title: "ê²€ì°°/ê²½ì°° ì‚¬ì¹­",
    rounds: [
      {
        round: 1,
        question: "ì•ˆë…•í•˜ì„¸ìš”. ì„œìš¸ ì§€ë°© ê²€ì°°ì²­ ê¹€ì¢…ì˜ ìˆ˜ì‚¬ê´€ì…ë‹ˆë‹¤. í˜„ì¬ ë³¸ì¸ ëª…ì˜ë¡œ ëŒ€í¬í†µì¥ì´ ê°œì„¤ë˜ì–´ ì—°ë½ë“œë ¸ìŠµë‹ˆë‹¤. ê¹€ì¢…ì˜ì”¨ ë§ìœ¼ì‹ ê°€ìš”?",
        audio_url: "https://voiceofinha-dev-bucket.s3.ap-northeast-2.amazonaws.com/scenario/%E1%84%80%E1%85%A5%E1%86%B7%E1%84%8E%E1%85%A1%E1%86%AF%E1%84%8E%E1%85%A5%E1%86%BC1.mp3"
      },
      {
        round: 2,
        question: "ì˜ˆ, ì§€ê¸ˆ ë³¸ì¸ ëª…ì˜ë¡œ ëœ ëŒ€í¬í†µì¥ì´ ë°œê²¬ë˜ì—ˆìœ¼ë‹ˆ, ë¹¨ë¦¬ ì¡°ì·¨ë¥¼ ì·¨í•´ì•¼ í•©ë‹ˆë‹¤â€¦",
        audio_url: "https://voiceofinha-dev-bucket.s3.ap-northeast-2.amazonaws.com/scenario/%E1%84%80%E1%85%A5%E1%86%B7%E1%84%8E%E1%85%A1%E1%86%AF%E1%84%8E%E1%85%A5%E1%86%BC2.mp3"
      },
      {
        round: 3,
        question: "ì§€ê¸ˆ ì €í¬ ê²€ì°°ì²­ í™ˆí˜ì´ì§€ì— ë“¤ì–´ê°€ì…”ì„œ ì´ë¦„ê³¼ ì£¼ë¯¼ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ë©´â€¦",
        audio_url: "https://voiceofinha-dev-bucket.s3.ap-northeast-2.amazonaws.com/scenario/%E1%84%80%E1%85%A5%E1%86%B7%E1%84%8E%E1%85%A1%E1%86%AF%E1%84%8E%E1%85%A5%E1%86%BC3.mp3"
      }
    ],
    guideline: "ê²½ì°°ì„œì—ì„œëŠ” ëŒ€í¬í†µì¥ ê´€ë ¨ ì „í™”ë¥¼ ê±¸ì§€ ì•ŠìŠµë‹ˆë‹¤. \në³´ì´ìŠ¤í”¼ì‹± ë²”ì£„ì˜ ì „í˜•ì ì¸ ìˆ˜ë²• ì¤‘ í•˜ë‚˜ê°€ \"ìì‹ ì„ ê²½ì°°, ê²€ì°°ì´ë¼ê³  ì‚¬ì¹­í•˜ë©° ëŒ€í¬í†µì¥ê³¼ ê´€ë ¨ëœ ì „í™”ë¥¼ ê±°ëŠ” ê²ƒ\"ì…ë‹ˆë‹¤. \në§Œì•½ ê²½ì°°ì„œë‚˜ ê²€ì°°ì²­ì´ë¼ê³  ì†ì´ëŠ” ì „í™”ë¥¼ ë°›ì•˜ë‹¤ë©´, í•´ë‹¹ ê¸°ê´€ì˜ ê³µì‹ ì „í™”ë²ˆí˜¸ë¡œ ì§ì ‘ ì „í™”í•˜ì—¬ ì‚¬ì‹¤ ì—¬ë¶€ë¥¼ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤.\n"
  };

  // ì‹œë‚˜ë¦¬ì˜¤ ìƒíƒœ
  const [scenario, setScenario] = useState<Scenario | null>(null);
  const [isLoadingScenario, setIsLoadingScenario] = useState(false);
  const [scenarioError, setScenarioError] = useState<string | null>(null);

  // í”Œë ˆì´ ìƒíƒœ
  const [currentRound, setCurrentRound] = useState(0);
  const [phase, setPhase] = useState<"preparing" | "playing" | "listening" | "processing" | "completed">("preparing");
  const [userResponses, setUserResponses] = useState<UserResponse[]>([]);
  const [isLoading, setIsLoading] = useState(false);
  const [isAudioReady, setIsAudioReady] = useState(false);

  // Refs
  const audioRef = useRef<HTMLAudioElement | null>(null);
  const audioContextRef = useRef<AudioContext | null>(null);
  const streamRef = useRef<MediaStream | null>(null);
  const vadIntervalRef = useRef<ReturnType<typeof setInterval> | null>(null);
  const recordedChunksRef = useRef<Blob[]>([]);

  // ë”ë¯¸ ì‹œë‚˜ë¦¬ì˜¤ ì„¤ì •
  useEffect(() => {
    console.log("ë”ë¯¸ ì‹œë‚˜ë¦¬ì˜¤ ì„¤ì •");
    setScenario(dummyScenario);
  }, []);

  // WAV ë…¹ìŒ ì‹œì‘
  const startWavRecording = useCallback(() => {
    if (!audioContextRef.current || !streamRef.current) {
      throw new Error('ì˜¤ë””ì˜¤ ì»¨í…ìŠ¤íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ')
    }

    // ë…¹ìŒ ë°ì´í„° ì´ˆê¸°í™”
    recordedChunksRef.current = [];

    // AudioContext ì„¤ì •
    const audioContext = audioContextRef.current;
    const source = audioContext.createMediaStreamSource(streamRef.current);

    // ScriptProcessorNode ì„¤ì •
    const scriptProcessor = audioContext.createScriptProcessor(4096, 1, 1);
    
    const audioData: number[] = []
    
    scriptProcessor.onaudioprocess = (e) => {
      const input = e.inputBuffer.getChannelData(0)
      for (let i = 0; i < input.length; i++) {
        audioData.push(input[i])
      }
    }
    
    source.connect(scriptProcessor)
    scriptProcessor.connect(audioContext.destination)
    
    // ë…¹ìŒ ë°ì´í„° ì €ì¥
    window.currentWavRecording = {
      audioData,
      processor: scriptProcessor,
      source,
      sampleRate: audioContext.sampleRate
    }

    console.log("WAV ë…¹ìŒ ì‹œì‘");
  }, []);

  // WAV ë…¹ìŒ ì¤‘ë‹¨
  const stopWavRecording = useCallback((): Blob | null => {
    const recording = window.currentWavRecording
    if (!recording) return null
    
    try {
      recording.source.disconnect()
      recording.processor.disconnect()
      
      // WAV íŒŒì¼ ìƒì„±
      const wavBuffer = createWavBuffer(recording.audioData, recording.sampleRate)
      const wavBlob = new Blob([wavBuffer], { type: 'audio/wav' })
      
      // ì •ë¦¬
      window.currentWavRecording = null
      
      console.log("WAV ë…¹ìŒ ì¤‘ë‹¨, íŒŒì¼ í¬ê¸°:", wavBlob.size);
      return wavBlob
    } catch (error) {
      console.error('WAV ë…¹ìŒ ì¤‘ë‹¨ ì‹¤íŒ¨:', error)
      return null
    }
  }, []);

  // WAV ë²„í¼ ìƒì„±
  const createWavBuffer = useCallback((audioData: number[], sampleRate: number): ArrayBuffer => {
    const length = audioData.length
    const buffer = new ArrayBuffer(44 + length * 2)
    const view = new DataView(buffer)
    
    // WAV í—¤ë” ì‘ì„±
    const writeString = (offset: number, string: string) => {
      for (let i = 0; i < string.length; i++) {
        view.setUint8(offset + i, string.charCodeAt(i))
      }
    }
    
    writeString(0, 'RIFF')
    view.setUint32(4, 36 + length * 2, true)
    writeString(8, 'WAVE')
    writeString(12, 'fmt ')
    view.setUint32(16, 16, true)
    view.setUint16(20, 1, true)
    view.setUint16(22, 1, true) // ëª¨ë…¸
    view.setUint32(24, sampleRate, true)
    view.setUint32(28, sampleRate * 2, true)
    view.setUint16(32, 2, true)
    view.setUint16(34, 16, true)
    writeString(36, 'data')
    view.setUint32(40, length * 2, true)
    
    // ì˜¤ë””ì˜¤ ë°ì´í„° ì‘ì„±
    let offset = 44
    for (let i = 0; i < length; i++) {
      const sample = Math.max(-1, Math.min(1, audioData[i]))
      view.setInt16(offset, sample < 0 ? sample * 0x8000 : sample * 0x7FFF, true)
      offset += 2
    }
    
    return buffer
  }, []);

  // ëª¨ë“  ì‘ë‹µì„ ìˆ˜ì§‘í•˜ì—¬ ë¶„ì„í•˜ëŠ” í•¨ìˆ˜ (ë”ë¯¸ êµ¬í˜„)
  const analyzeAllResponses = useCallback(async () => {
    if (!scenario) return;
    
    try {
      setPhase("processing");
      setIsLoading(true);

      console.log("ìµœì¢… ë¶„ì„ ì¤‘... (ë”ë¯¸ êµ¬í˜„)");
      
      // ë”ë¯¸ ë¶„ì„ ê²°ê³¼
      const dummyAnalysis = `
## ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ ë¶„ì„

**ì‹œë‚˜ë¦¬ì˜¤**: ${scenario.title}

**ì´ ë¼ìš´ë“œ**: ${scenario.rounds.length}
**ì™„ë£Œëœ ì‘ë‹µ**: ${userResponses.length}

### ì‘ë‹µ ë¶„ì„
${userResponses.map((r, idx) => `
**ë¼ìš´ë“œ ${r.round}**
- ì‘ë‹µ: ${r.transcription || "[ìŒì„± ë³€í™˜ ì™„ë£Œ]"}
- ìƒíƒœ: âœ… ì™„ë£Œ
`).join('\n')}

### ì¢…í•© í‰ê°€
ë³´ì´ìŠ¤í”¼ì‹± ëŒ€ì‘ ëŠ¥ë ¥ì´ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤. 
ì‹¤ì œ ìƒí™©ì—ì„œëŠ” ì¦‰ì‹œ ì „í™”ë¥¼ ëŠê³  ê³µì‹ ê¸°ê´€ì— í™•ì¸ ì „í™”ë¥¼ í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.

**ì¶”ì²œ í–‰ë™ìš”ë ¹**:
1. ê²€ì°°/ê²½ì°°ì„ ì‚¬ì¹­í•˜ëŠ” ì „í™”ëŠ” ì¦‰ì‹œ ëŠê¸°
2. ê°œì¸ì •ë³´ ì ˆëŒ€ ì œê³µ ê¸ˆì§€  
3. ê³µì‹ ê¸°ê´€ ë²ˆí˜¸ë¡œ ì§ì ‘ í™•ì¸
4. ê°€ì¡±/ì§€ì¸ê³¼ ìƒì˜í•˜ê¸°
      `;

      // ë”ë¯¸ ê²°ê³¼ë¥¼ sessionStorageì— ì €ì¥
      const finalResult = {
        scenario: {
          id: scenario.id,
          title: scenario.title,
          rounds: scenario.rounds
        },
        userResponses: userResponses,
        analysis: dummyAnalysis,
        timestamp: new Date().toISOString()
      };
      
      sessionStorage.setItem("simulationResult", JSON.stringify(finalResult));

      setPhase("completed");
      
      // ê²°ê³¼ í˜ì´ì§€ë¡œ ì´ë™
      setTimeout(() => {
        router.push("/simulation/results");
      }, 2000);

    } catch (error) {
      console.error("ìµœì¢… ë¶„ì„ ì‹¤íŒ¨:", error);
      alert(`ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: ${error instanceof Error ? error.message : "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"}`);
      setPhase("preparing");
    } finally {
      setIsLoading(false);
    }
  }, [scenario, userResponses, router]);

  // ë…¹ìŒ ì™„ë£Œ â†’ STT (ë”ë¯¸ êµ¬í˜„)
  const handleRecordingComplete = useCallback(async (audioBlob: Blob) => {
    try {
      setIsLoading(true);
      console.log(`ë¼ìš´ë“œ ${currentRound + 1} ìŒì„± ì²˜ë¦¬ ì¤‘... (ë”ë¯¸ STT)`);

      // ë”ë¯¸ STT ê²°ê³¼
      const dummyTranscripts = [
        "ì•ˆë…•í•˜ì„¸ìš”. ì €ëŠ” ê·¸ëŸ° í†µì¥ì„ ë§Œë“  ì ì´ ì—†ëŠ”ë°ìš”?",
        "ì •ë§ ê²€ì°°ì²­ì—ì„œ ì „í™”í•˜ì‹  ê²Œ ë§ë‚˜ìš”? í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.",
        "ê°œì¸ì •ë³´ëŠ” ì „í™”ë¡œ ë§ì”€ë“œë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì§ì ‘ ë°©ë¬¸í•˜ê² ìŠµë‹ˆë‹¤."
      ];

      const dummyTranscript = dummyTranscripts[currentRound] || "ë„¤, ì•Œê² ìŠµë‹ˆë‹¤.";

      // ë”ë¯¸ ì§€ì—°
      await new Promise(resolve => setTimeout(resolve, 2000));

      // ì‚¬ìš©ì ì‘ë‹µ ì €ì¥
      const userResponse: UserResponse = {
        round: currentRound + 1,
        audioBlob: audioBlob,
        transcription: dummyTranscript,
      };
      
      setUserResponses(prev => [...prev, userResponse]);

      // ë‹¤ìŒ ë¼ìš´ë“œë¡œ ì§„í–‰
      const nextRound = currentRound + 1;
      console.log(`í˜„ì¬ ë¼ìš´ë“œ: ${currentRound + 1}, ë‹¤ìŒ ë¼ìš´ë“œ: ${nextRound + 1}, ì´ ë¼ìš´ë“œ: ${scenario!.rounds.length}`);
      
      if (nextRound < scenario!.rounds.length) {
        console.log(`ë¼ìš´ë“œ ${nextRound + 1} ì‹œì‘ ì˜ˆì •...`);
        setCurrentRound(nextRound);
      } else {
        // ëª¨ë“  ë¼ìš´ë“œ ì™„ë£Œì‹œ ë¶„ì„ ì‹œì‘
        console.log("ëª¨ë“  ë¼ìš´ë“œ ì™„ë£Œ! ìµœì¢… ë¶„ì„ ì‹œì‘...");
        await analyzeAllResponses();
      }

    } catch (error) {
      console.error("STT ì²˜ë¦¬ ì‹¤íŒ¨:", error);
      const msg = error instanceof Error ? error.message : "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜";
      if (confirm(`ìŒì„± ì¸ì‹ ì‹¤íŒ¨: ${msg}\në‹¤ì‹œ ì‹œë„í• ê¹Œìš”?`)) {
        setPhase("listening");
        startListening();
      } else {
        setPhase("preparing");
        alert("ë‹¤ì‹œ ì‹œë„í•˜ë ¤ë©´ í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•˜ì„¸ìš”.");
      }
    } finally {
      setIsLoading(false);
    }
  }, [currentRound, scenario, analyzeAllResponses]);

  // ë¦¬ìŠ¤ë‹ ì¤‘ë‹¨
  const stopListening = useCallback(() => {
    console.log("ë¦¬ìŠ¤ë‹ ì¤‘ë‹¨");
    setPhase("processing");
    
    if (vadIntervalRef.current) {
      clearInterval(vadIntervalRef.current);
      vadIntervalRef.current = null;
    }

    // VAD ìƒíƒœ ë¦¬ì…‹
    VoiceActivityDetector.reset();

    // MediaRecorder ë…¹ìŒ ì¤‘ë‹¨
    if (window.currentWavRecording) {
      const wavBlob = stopWavRecording();
      if (wavBlob) {
        handleRecordingComplete(wavBlob);
      } else {
        console.error("WAV ë…¹ìŒ ì¤‘ë‹¨ë˜ì§€ ì•ŠìŒ");
        setPhase("listening");
      }
    } else {
      console.error("WAV ë…¹ìŒì´ ì¤‘ë‹¨ë˜ì§€ ì•ŠìŒ");
      setPhase("listening");
    }
  }, [stopWavRecording, handleRecordingComplete]);

  // ë¦¬ìŠ¤ë‹ ì‹œì‘
  const startListening = useCallback(() => {
    console.log("ìŒì„± ì¸ì‹ ì‹œì‘");
    setPhase("listening");
    
    if (!VoiceActivityDetector.isInitialized) {
      console.error("VADê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ");
      return;
    }

    // VAD ìƒíƒœ ë¦¬ì…‹
    VoiceActivityDetector.reset();

    try {
      // WAV ì§ì ‘ ë…¹ìŒ ì‹œì‘
      startWavRecording();
      console.log("WAV ë…¹ìŒ ì‹œì‘ë¨");
    } catch (error) {
      console.error("ë…¹ìŒ ì‹œì‘ ì‹¤íŒ¨:", error);
      return;
    }

    // ê°œì„ ëœ VAD ë£¨í”„
    vadIntervalRef.current = setInterval(() => {
      // ìŒì„± ì¢…ë£Œ ì²´í¬
      if (VoiceActivityDetector.checkSpeechEnd()) {
        console.log("VAD: ìŒì„± ì¢…ë£Œ ê°ì§€, ë…¹ìŒ ì¤‘ë‹¨");
        stopListening();
      }
    }, 100); // 100ms ê°„ê²©ìœ¼ë¡œ ì²´í¬

    // í•˜ë“œ íƒ€ì„ì•„ì›ƒ (ì•ˆì „ì¥ì¹˜ - 30ì´ˆ)
    setTimeout(() => {
      if (window.currentWavRecording) {
        console.log("íƒ€ì„ì•„ì›ƒ ë„ë‹¬, ê°•ì œ ë…¹ìŒ ì¤‘ë‹¨");
        stopListening();
      }
    }, 30_000);
  }, [startWavRecording, stopListening]);

  // í˜„ì¬ ë¼ìš´ë“œ ì‹œì‘
  const startCurrentRound = useCallback(async () => {
    if (!scenario) return;

    console.log(`=== ë¼ìš´ë“œ ${currentRound + 1} ì‹œì‘ (ì´ ${scenario.rounds.length}ë¼ìš´ë“œ) ===`);

    // ëª¨ë“  ë¼ìš´ë“œ ì™„ë£Œ ì²´í¬
    if (currentRound >= scenario.rounds.length) {
      console.log("ëª¨ë“  ë¼ìš´ë“œ ì™„ë£Œ! ìµœì¢… ë¶„ì„ ì‹œì‘...");
      await analyzeAllResponses();
      return;
    }

    setPhase("playing");
    const round = scenario.rounds[currentRound];

    console.log(`ë¼ìš´ë“œ ${currentRound + 1} ì§ˆë¬¸:`, round.question);
    console.log(`ë¼ìš´ë“œ ${currentRound + 1} ì˜¤ë””ì˜¤ URL:`, round.audio_url);

    const audio = new Audio(round.audio_url);
    audioRef.current = audio;

    audio.onended = () => {
      console.log(`ë¼ìš´ë“œ ${currentRound + 1} ì˜¤ë””ì˜¤ ì¬ìƒ ì™„ë£Œ, ë¦¬ìŠ¤ë‹ ì‹œì‘`);
      // ì•½ê°„ì˜ ë”œë ˆì´ í›„ ë¦¬ìŠ¤ë‹ ì‹œì‘
      setTimeout(() => {
        startListening();
      }, 500);
    };
    
    audio.onerror = (e) => {
      console.error("ì˜¤ë””ì˜¤ ì¬ìƒ ì‹¤íŒ¨:", e);
      alert("ì˜¤ë””ì˜¤ë¥¼ ì¬ìƒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¦¬ìŠ¤ë‹ìœ¼ë¡œ ë°”ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.");
      setTimeout(() => {
        startListening();
      }, 500);
    };

    try {
      await audio.play();
      console.log(`ë¼ìš´ë“œ ${currentRound + 1} ì˜¤ë””ì˜¤ ì¬ìƒ ì‹œì‘`);
    } catch (e) {
      console.error("ì˜¤ë””ì˜¤ ì¬ìƒ ì˜¤ë¥˜:", e);
      // ìë™ì¬ìƒ ì œí•œ ë“±ì˜ ê²½ìš° ë°”ë¡œ ë¦¬ìŠ¤ë‹ìœ¼ë¡œ ì§„í–‰
      console.log("ì˜¤ë””ì˜¤ ìë™ì¬ìƒ ì‹¤íŒ¨, ë¦¬ìŠ¤ë‹ìœ¼ë¡œ ë°”ë¡œ ì§„í–‰");
      setTimeout(() => {
        startListening();
      }, 500);
    }
  }, [scenario, currentRound, analyzeAllResponses, startListening]);

  // ì˜¤ë””ì˜¤ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
  const initializeAudio = useCallback(async () => {
    console.log("ì˜¤ë””ì˜¤ ì´ˆê¸°í™” ì‹œì‘...");
    
    try {
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 16000,
          channelCount: 1,
        },
      });
      streamRef.current = stream;

      // AudioContext (í¬ë¡œìŠ¤ë¸Œë¼ìš°ì €)
      const ACtor = window.AudioContext || window.webkitAudioContext!;
      audioContextRef.current = new ACtor();
      
      if (audioContextRef.current.state === "suspended") {
        await audioContextRef.current.resume();
      }
      
      // VAD ì´ˆê¸°í™”
      const source = audioContextRef.current.createMediaStreamSource(stream);
      VoiceActivityDetector.init(audioContextRef.current, source);

      console.log("ì˜¤ë””ì˜¤ ì´ˆê¸°í™” ì™„ë£Œ");
      return true;
    } catch (error) {
      console.error("ì˜¤ë””ì˜¤ ì´ˆê¸°í™” ì‹¤íŒ¨:", error);
      throw error;
    }
  }, []);

  // ì •ë¦¬
  const cleanup = useCallback(() => {
    console.log("ì˜¤ë””ì˜¤ ì •ë¦¬ ì‹œì‘");
    
    if (vadIntervalRef.current) {
      clearInterval(vadIntervalRef.current);
      vadIntervalRef.current = null;
    }
    
    if (audioRef.current) {
      audioRef.current.pause();
      audioRef.current = null;
    }
    
    if (window.currentWavRecording) {
      window.currentWavRecording = null;
    }
    
    if (streamRef.current) {
      streamRef.current.getTracks().forEach((t) => t.stop());
      streamRef.current = null;
    }
    
    if (audioContextRef.current && audioContextRef.current.state !== "closed") {
      audioContextRef.current.close();
      audioContextRef.current = null;
    }
    
    VoiceActivityDetector.cleanup();
    console.log("ì˜¤ë””ì˜¤ ì •ë¦¬ ì™„ë£Œ");
  }, []);

  // ì˜¤ë””ì˜¤ ì´ˆê¸°í™”
  useEffect(() => {
    const init = async () => {
      try {
        await initializeAudio();
        setIsAudioReady(true);
      } catch (error) {
        console.error("ì˜¤ë””ì˜¤ ì´ˆê¸°í™” ì‹¤íŒ¨:", error);
        alert("ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.");
        setIsAudioReady(false);
      }
    };
    init();
    return () => cleanup();
  }, [initializeAudio, cleanup]);

  // ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘
  useEffect(() => {
    if (isAudioReady && scenario && currentRound === 0) {
      console.log("=== ì‹œë®¬ë ˆì´ì…˜ ì‹œì‘ ===");
      console.log("ì‹œë‚˜ë¦¬ì˜¤ ì •ë³´:", {
        id: scenario.id,
        title: scenario.title,
        totalRounds: scenario.rounds.length
      });
      
      // ì•½ê°„ì˜ ë”œë ˆì´ í›„ ì‹œì‘
      setTimeout(() => {
        startCurrentRound();
      }, 1000);
    }
  }, [isAudioReady, scenario, currentRound, startCurrentRound]);

  // currentRound ë³€í™” ê°ì§€í•˜ì—¬ ë‹¤ìŒ ë¼ìš´ë“œ ì‹œì‘
  useEffect(() => {
    if (scenario && currentRound > 0 && currentRound < scenario.rounds.length) {
      console.log(`=== currentRound ìƒíƒœ ë³€í™” ê°ì§€: ${currentRound} ===`);
      console.log(`ìë™ìœ¼ë¡œ ë¼ìš´ë“œ ${currentRound + 1} ì‹œì‘...`);
      
      // ì§§ì€ ë”œë ˆì´ í›„ ë¼ìš´ë“œ ì‹œì‘
      setTimeout(() => {
        startCurrentRound();
      }, 1000);
    }
  }, [currentRound, scenario, startCurrentRound]);

  // ìˆ˜ë™ í…ŒìŠ¤íŠ¸ ë²„íŠ¼ë“¤ (ê°œë°œìš©)
  const handleManualNext = () => {
    if (scenario && currentRound < scenario.rounds.length - 1) {
      console.log("ìˆ˜ë™ìœ¼ë¡œ ë‹¤ìŒ ë¼ìš´ë“œë¡œ ì§„í–‰");
      setCurrentRound(prev => prev + 1);
    } else if (scenario && currentRound === scenario.rounds.length - 1) {
      console.log("ìˆ˜ë™ìœ¼ë¡œ ì‹œë®¬ë ˆì´ì…˜ ì™„ë£Œ");
      analyzeAllResponses();
    }
  };

  const handleManualStop = () => {
    console.log("ìˆ˜ë™ìœ¼ë¡œ ë¦¬ìŠ¤ë‹ ì¤‘ë‹¨");
    if (phase === "listening") {
      stopListening();
    }
  };

  const getPhaseMessage = () => {
    switch (phase) {
      case "preparing":
        return "ì‹œë®¬ë ˆì´ì…˜ì„ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤...";
      case "playing":
        return `ë¼ìš´ë“œ ${currentRound + 1}: ìƒëŒ€ë°©ì´ ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤...`;
      case "listening":
        return "ğŸ¤ ë‹¹ì‹ ì˜ ì‘ë‹µì„ ê¸°ë‹¤ë¦¬ê³  ìˆìŠµë‹ˆë‹¤. ë§ì”€í•´ì£¼ì„¸ìš”!";
      case "processing":
        return "ì‘ë‹µì„ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤...";
      case "completed":
        return "ì‹œë®¬ë ˆì´ì…˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!";
      default:
        return "";
    }
  };

  const getPhaseIcon = () => {
    switch (phase) {
      case "preparing":
        return "âš™ï¸";
      case "playing":
        return "ğŸ“";
      case "listening":
        return "ğŸ¤";
      case "processing":
        return "â³";
      case "completed":
        return "âœ…";
      default:
        return "";
    }
  };

  const getVolumeLevel = () => {
    if (VoiceActivityDetector.isInitialized) {
      const volume = VoiceActivityDetector.getVolume();
      const percentage = Math.min(100, (volume / 50) * 100); // 50ì„ ìµœëŒ€ê°’ìœ¼ë¡œ ê°€ì •
      return percentage;
    }
    return 0;
  };

  return (
    <div className="min-h-screen bg-black flex items-center justify-center p-4">
      <div className="max-w-2xl w-full">
        {/* í—¤ë” */}
        <div className="text-center mb-8">
          <h1 className="text-3xl font-bold text-white mb-2">ë³´ì´ìŠ¤í”¼ì‹± ì‹œë®¬ë ˆì´ì…˜</h1>

          {isLoadingScenario && (
            <div className="text-center">
              <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-white mx-auto mb-4" />
              <p className="text-gray-400">ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...</p>
            </div>
          )}

          {scenarioError && (
            <div className="text-center">
              <p className="text-red-400 mb-4">âš ï¸ {scenarioError}</p>
              <button
                onClick={() => window.location.reload()}
                className="bg-red-600 hover:bg-red-700 text-white px-4 py-2 rounded-lg"
              >
                ë‹¤ì‹œ ì‹œë„
              </button>
            </div>
          )}

          {scenario && !isLoadingScenario && !scenarioError && (
            <>
              <h2 className="text-xl text-gray-300 mb-4">{scenario.title}</h2>
              <div className="flex items-center justify-center space-x-2">
                <span className="text-4xl">{getPhaseIcon()}</span>
                <p className="text-lg text-gray-400">{getPhaseMessage()}</p>
              </div>
            </>
          )}
        </div>

        {!scenario || isLoadingScenario || scenarioError ? (
          <div className="text-center">
            {isLoadingScenario && (
              <div className="bg-gray-900 border border-gray-700 rounded-lg p-6">
                <div className="animate-spin rounded-full h-12 w-12 border-b-2 border-white mx-auto mb-4" />
                <p className="text-gray-400">ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤...</p>
              </div>
            )}
          </div>
        ) : (
          <>
            {/* ì§„í–‰ ìƒí™© */}
            <div className="bg-gray-900 border border-gray-700 rounded-lg p-6 mb-8">
              <div className="flex items-center justify-between mb-4">
                <span className="text-gray-400">ì§„í–‰ ìƒí™©</span>
                <span className="text-white">
                  {Math.min(currentRound + 1, scenario.rounds.length)} / {scenario.rounds.length}
                </span>
              </div>
              <div className="w-full bg-gray-700 rounded-full h-2">
                <div
                  className="bg-blue-600 h-2 rounded-full transition-all duration-500"
                  style={{
                    width: `${((currentRound + 1) / scenario.rounds.length) * 100}%`,
                  }}
                />
              </div>
            </div>

            {/* í˜„ì¬ ë¼ìš´ë“œ */}
            {currentRound < scenario.rounds.length && (
              <div className="bg-gray-900 border border-gray-700 rounded-lg p-6 mb-8">
                <div className="flex items-center space-x-3 mb-4">
                  <span className="bg-blue-600 text-white px-3 py-1 rounded-full text-sm font-medium">
                    ë¼ìš´ë“œ {currentRound + 1}
                  </span>
                </div>

                {phase === "playing" && (
                  <div className="text-gray-300">
                    <p className="mb-2">ğŸ“ ìƒëŒ€ë°©:</p>
                    <p className="text-lg italic border-l-4 border-red-500 pl-4">
                      &ldquo;{scenario.rounds[currentRound].question}&rdquo;
                    </p>
                  </div>
                )}

                {phase === "listening" && (
                  <div className="text-center">
                    <div className="animate-pulse mb-4">
                      <div className="w-16 h-16 bg-red-600 rounded-full mx-auto flex items-center justify-center">
                        <span className="text-2xl">ğŸ¤</span>
                      </div>
                    </div>
                    <p className="text-white text-lg mb-2">ìŒì„±ì„ ê°ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤</p>
                    <p className="text-gray-400 text-sm mb-4">ë§ì”€ì´ ëë‚˜ë©´ ìë™ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰ë©ë‹ˆë‹¤</p>
                    
                    {/* ìŒì„± ë ˆë²¨ í‘œì‹œ */}
                    <div className="bg-gray-800 rounded-lg p-4">
                      <p className="text-gray-400 text-sm mb-2">ìŒì„± ë ˆë²¨</p>
                      <div className="w-full bg-gray-700 rounded-full h-3">
                        <div
                          className={`h-3 rounded-full transition-all duration-200 ${
                            getVolumeLevel() > VoiceActivityDetector.VOLUME_THRESHOLD 
                              ? 'bg-green-500' 
                              : 'bg-gray-600'
                          }`}
                          style={{ width: `${Math.min(100, getVolumeLevel())}%` }}
                        />
                      </div>
                      <div className="flex justify-between text-xs text-gray-500 mt-1">
                        <span>ì¡°ìš©í•¨</span>
                        <span>ì„ê³„ê°’: {VoiceActivityDetector.VOLUME_THRESHOLD}</span>
                        <span>í¼</span>
                      </div>
                    </div>

                    {/* í…ŒìŠ¤íŠ¸ ë²„íŠ¼ë“¤ */}
                    <div className="mt-4 space-x-2">
                      <button
                        onClick={handleManualStop}
                        className="bg-orange-600 hover:bg-orange-700 text-white px-4 py-2 rounded-lg text-sm"
                      >
                        ìˆ˜ë™ ì¤‘ë‹¨
                      </button>
                      <button
                        onClick={handleManualNext}
                        className="bg-green-600 hover:bg-green-700 text-white px-4 py-2 rounded-lg text-sm"
                      >
                        ë‹¤ìŒ ë¼ìš´ë“œ
                      </button>
                    </div>
                  </div>
                )}
              </div>
            )}

            {/* ë¡œë”© */}
            {isLoading && (
              <div className="bg-gray-900 border border-gray-700 rounded-lg p-6 text-center">
                <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-white mx-auto mb-4" />
                <p className="text-gray-400">ì²˜ë¦¬ ì¤‘...</p>
              </div>
            )}

            {/* ì™„ë£Œëœ ë¼ìš´ë“œ ë¯¸ë¦¬ë³´ê¸° */}
            {userResponses.length > 0 && (
              <div className="bg-gray-900 border border-gray-700 rounded-lg p-6 mb-8">
                <h3 className="text-lg font-semibold text-white mb-4">ì™„ë£Œëœ ì‘ë‹µ</h3>
                <div className="space-y-3">
                  {userResponses.map((r, idx) => (
                    <div key={idx} className="bg-gray-800 p-3 rounded-lg">
                      <div className="flex items-center space-x-2 mb-2">
                        <span className="bg-green-600 text-white px-2 py-1 rounded text-xs">ë¼ìš´ë“œ {r.round}</span>
                        <span className="text-green-400">âœ“</span>
                      </div>
                      <p className="text-gray-300 text-sm">{r.transcription || "ìŒì„± ë³€í™˜ ì¤‘..."}</p>
                    </div>
                  ))}
                </div>
              </div>
            )}

            {/* ë””ë²„ê·¸ ì •ë³´ */}
            {process.env.NODE_ENV === 'development' && (
              <div className="bg-gray-900 border border-gray-700 rounded-lg p-4 mb-8">
                <h3 className="text-sm font-semibold text-white mb-2">ë””ë²„ê·¸ ì •ë³´</h3>
                <div className="text-xs text-gray-400 space-y-1">
                  <p>í˜„ì¬ í˜ì´ì¦ˆ: {phase}</p>
                  <p>í˜„ì¬ ë¼ìš´ë“œ: {currentRound}</p>
                  <p>ì˜¤ë””ì˜¤ ì¤€ë¹„: {isAudioReady ? 'âœ…' : 'âŒ'}</p>
                  <p>VAD ì´ˆê¸°í™”: {VoiceActivityDetector.isInitialized ? 'âœ…' : 'âŒ'}</p>
                  <p>ìŒì„± ê°ì§€ ì¤‘: {VoiceActivityDetector.isSpeaking ? 'âœ…' : 'âŒ'}</p>
                  <p>í˜„ì¬ ë³¼ë¥¨: {VoiceActivityDetector.isInitialized ? VoiceActivityDetector.getVolume().toFixed(1) : 'N/A'}</p>
                </div>
              </div>
            )}

            <div className="mt-8 text-center">
              <p className="text-gray-500 text-sm">ì´ ì‹œë®¬ë ˆì´ì…˜ì€ ë³´ì´ìŠ¤í”¼ì‹± ëŒ€ì‘ ëŠ¥ë ¥ í–¥ìƒì„ ìœ„í•œ ì—°ìŠµì…ë‹ˆë‹¤.</p>
            </div>
          </>
        )}
      </div>
    </div>
  );
}