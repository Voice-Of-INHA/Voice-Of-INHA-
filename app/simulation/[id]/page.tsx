"use client"

import { useEffect, useState, useRef } from "react"
import { useRouter } from "next/navigation"

// ì¸í„°í˜ì´ìŠ¤ ì •ì˜
interface Round {
  round: number
  question: string
  audio_url: string
}

interface Scenario {
  id: number
  title: string
  rounds: Round[]
  guideline: string
}

interface UserResponse {
  round: number
  audioBlob: Blob
  transcription?: string
}

// WebKit AudioContext íƒ€ì… ì •ì˜
interface WebKitWindow extends Window {
  webkitAudioContext: typeof AudioContext
}

// VAD (Voice Activity Detection) ëª¨ë“ˆ
const VoiceActivityDetector = {
  // VAD ì„¤ì •
  VOLUME_THRESHOLD: 30, // ìŒì„± ê°ì§€ ì„ê³„ê°’ (ì¡°ì • ê°€ëŠ¥)
  SILENCE_DURATION: 5000, // ì¹¨ë¬µ ì§€ì† ì‹œê°„ (ms, ì¡°ì • ê°€ëŠ¥)
  
  analyser: null as AnalyserNode | null,
  dataArray: null as Uint8Array | null,
  isInitialized: false,
  
  // VAD ì´ˆê¸°í™”
  init(audioContext: AudioContext, source: MediaStreamAudioSourceNode) {
    try {
      this.analyser = audioContext.createAnalyser()
      this.analyser.fftSize = 256
      this.analyser.smoothingTimeConstant = 0.8
      source.connect(this.analyser)
      
      const bufferLength = this.analyser.frequencyBinCount
      this.dataArray = new Uint8Array(bufferLength)
      this.isInitialized = true
      
      console.log('VAD ì´ˆê¸°í™” ì™„ë£Œ:', { bufferLength, fftSize: this.analyser.fftSize })
    } catch (error) {
      console.error('VAD ì´ˆê¸°í™” ì‹¤íŒ¨:', error)
      this.isInitialized = false
    }
  },
  
  // í˜„ì¬ ë³¼ë¥¨ ì¸¡ì •
  getVolume(): number {
    if (!this.isInitialized || !this.analyser || !this.dataArray) {
      return 0
    }
    
    try {
      let sum = 0
      for (let i = 0; i < this.dataArray.length; i++) {
        sum += this.dataArray[i]
      }
      // í‰ê·  ë³¼ë¥¨ ë°˜í™˜
      return sum / this.dataArray.length
    } catch (error) {
      console.error('ë³¼ë¥¨ ì¸¡ì • ì˜¤ë¥˜:', error)
      return 0
    }
  },
  
  // ìŒì„± ê°ì§€ ì—¬ë¶€
  isVoiceDetected(): boolean {
    const volume = this.getVolume()
    return volume > this.VOLUME_THRESHOLD
  },
  
  // VAD ì •ë¦¬
  cleanup() {
    this.analyser = null
    this.dataArray = null
    this.isInitialized = false
  }
}

// ë©”ì¸ ì»´í¬ë„ŒíŠ¸
export default function SimulationPage() {
  const router = useRouter()
  
  // ì‹œë‚˜ë¦¬ì˜¤ ë°ì´í„° (ì‹¤ì œë¡œëŠ” propsë‚˜ APIì—ì„œ ë°›ì•„ì˜¬ ê²ƒ)
  const [scenario] = useState<Scenario>({
    id: 1,
    title: "ê²€ì°°/ê²½ì°° ì‚¬ì¹­",
    rounds: [
      {
        round: 1,
        question: "ì•ˆë…•í•˜ì„¸ìš”. ì„œìš¸ ì§€ë°© ê²€ì°°ì²­ ê¹€ì¢…ì˜ ìˆ˜ì‚¬ê´€ì…ë‹ˆë‹¤. í˜„ì¬ ë³¸ì¸ ëª…ì˜ë¡œ ëŒ€í¬í†µì¥ì´ ê°œì„¤ë˜ì–´ ì—°ë½ë“œë ¸ìŠµë‹ˆë‹¤. ê¹€ì¢…ì˜ì”¨ ë§ìœ¼ì‹ ê°€ìš”?",
        audio_url: "https://voiceofinha-dev-bucket.s3.ap-northeast-2.amazonaws.com/scenario/%E1%84%80%E1%85%A5%E1%86%B7%E1%84%8E%E1%85%A1%E1%86%AF%E1%84%8E%E1%85%A5%E1%86%BC1.mp3"
      },
      {
        round: 2,
        question: "ì˜ˆ, ì§€ê¸ˆ ë³¸ì¸ ëª…ì˜ë¡œ ëœ ëŒ€í¬í†µì¥ì´ ë°œê²¬ë˜ì—ˆìœ¼ë‹ˆ, ë¹¨ë¦¬ ì¡°ì·¨ë¥¼ ì·¨í•´ì•¼ í•©ë‹ˆë‹¤â€¦",
        audio_url: "https://voiceofinha-dev-bucket.s3.ap-northeast-2.amazonaws.com/scenario/%E1%84%80%E1%85%A5%E1%86%B7%E1%84%8E%E1%85%A1%E1%86%AF%E1%84%8E%E1%85%A5%E1%86%BC2.mp3"
      },
      {
        round: 3,
        question: "ì§€ê¸ˆ ì €í¬ ê²€ì°°ì²­ í™ˆí˜ì´ì§€ì— ë“¤ì–´ê°€ì…”ì„œ ì´ë¦„ê³¼ ì£¼ë¯¼ë²ˆí˜¸ë¥¼ ì…ë ¥í•˜ë©´â€¦",
        audio_url: "https://voiceofinha-dev-bucket.s3.ap-northeast-2.amazonaws.com/scenario/%E1%84%80%E1%85%A5%E1%86%B7%E1%84%8E%E1%85%A1%E1%86%AF%E1%84%8E%E1%85%A5%E1%86%BC3.mp3"
      }
    ],
    guideline: "ê²½ì°°ì„œì—ì„œëŠ” ëŒ€í¬í†µì¥ ê´€ë ¨ ì „í™”ë¥¼ ê±¸ì§€ ì•ŠìŠµë‹ˆë‹¤. ë³´ì´ìŠ¤í”¼ì‹± ë²”ì£„ì˜ ì „í˜•ì ì¸ ìˆ˜ë²• ì¤‘ í•˜ë‚˜ê°€ \"ìì‹ ì„ ê²½ì°°, ê²€ì°°ì´ë¼ê³  ì‚¬ì¹­í•˜ë©° ëŒ€í¬í†µì¥ê³¼ ê´€ë ¨ëœ ì „í™”ë¥¼ ê±°ëŠ” ê²ƒ\"ì…ë‹ˆë‹¤."
  })

  // ìƒíƒœ ê´€ë¦¬
  const [currentRound, setCurrentRound] = useState(0)
  const [phase, setPhase] = useState<'preparing' | 'playing' | 'listening' | 'processing' | 'completed'>('preparing')
  const [userResponses, setUserResponses] = useState<UserResponse[]>([])
  const [isLoading, setIsLoading] = useState(false)
  const [isAudioReady, setIsAudioReady] = useState(false) // ì˜¤ë””ì˜¤ ì´ˆê¸°í™” ìƒíƒœ

  // Ref ê°ì²´
  const audioRef = useRef<HTMLAudioElement | null>(null)
  const mediaRecorderRef = useRef<MediaRecorder | null>(null)
  const audioContextRef = useRef<AudioContext | null>(null)
  const streamRef = useRef<MediaStream | null>(null)
  const silenceTimerRef = useRef<NodeJS.Timeout | null>(null)
  const vadIntervalRef = useRef<NodeJS.Timeout | null>(null)
  const recordedChunksRef = useRef<Blob[]>([])

  // ì»´í¬ë„ŒíŠ¸ ë§ˆìš´íŠ¸ ì‹œ ì˜¤ë””ì˜¤ ì´ˆê¸°í™”
  useEffect(() => {
    const init = async () => {
      try {
        await initializeAudio()
        setIsAudioReady(true)
      } catch (error) {
        console.error('ì˜¤ë””ì˜¤ ì´ˆê¸°í™” ì‹¤íŒ¨:', error)
        alert('ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.')
        setIsAudioReady(false)
      }
    }
    init()
    
    // ì»´í¬ë„ŒíŠ¸ ì–¸ë§ˆìš´íŠ¸ ì‹œ ì •ë¦¬
    return () => {
      cleanup()
    }
  }, [])
  
  // ì˜¤ë””ì˜¤ê°€ ì¤€ë¹„ë˜ê±°ë‚˜ ë¼ìš´ë“œê°€ ë³€ê²½ë  ë•Œë§ˆë‹¤ ë‹¤ìŒ ë¼ìš´ë“œ ì‹œì‘
  useEffect(() => {
    if (isAudioReady) {
      startCurrentRound()
    }
  }, [isAudioReady, currentRound])

  // ì˜¤ë””ì˜¤ ì‹œìŠ¤í…œ ì´ˆê¸°í™” í•¨ìˆ˜
  const initializeAudio = async () => {
    console.log('ì˜¤ë””ì˜¤ ì´ˆê¸°í™” ì‹œì‘...')
    
    const stream = await navigator.mediaDevices.getUserMedia({ 
      audio: {
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true
      } 
    })
    streamRef.current = stream
    console.log('ë§ˆì´í¬ ìŠ¤íŠ¸ë¦¼ íšë“ ì™„ë£Œ')

    audioContextRef.current = new window.AudioContext
    if (audioContextRef.current.state === 'suspended') {
      await audioContextRef.current.resume()
    }
    
    const source = audioContextRef.current.createMediaStreamSource(stream)
    VoiceActivityDetector.init(audioContextRef.current, source)
    console.log('VAD ì´ˆê¸°í™” ì™„ë£Œ')

    const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus') 
      ? 'audio/webm;codecs=opus' 
      : 'audio/webm'
      
    mediaRecorderRef.current = new MediaRecorder(stream, { mimeType })
    console.log('MediaRecorder ìƒì„± ì™„ë£Œ:', mimeType)

    mediaRecorderRef.current.ondataavailable = (event) => {
      if (event.data.size > 0) {
        recordedChunksRef.current.push(event.data)
      }
    }

    mediaRecorderRef.current.onstop = () => {
      const audioBlob = new Blob(recordedChunksRef.current, { type: mimeType })
      handleRecordingComplete(audioBlob)
      recordedChunksRef.current = []
    }

    mediaRecorderRef.current.onerror = (event) => {
      console.error('MediaRecorder ì˜¤ë¥˜:', event)
    }
  }

  // í˜„ì¬ ë¼ìš´ë“œ ì‹œì‘
  const startCurrentRound = async () => {
    // ëª¨ë“  ë¼ìš´ë“œê°€ ì™„ë£Œë˜ë©´ ë¶„ì„ ì‹œì‘
    if (currentRound >= scenario.rounds.length) {
      await analyzeResponses()
      return
    }

    setPhase('playing')
    const round = scenario.rounds[currentRound]
    
    // ì˜¤ë””ì˜¤ ì¬ìƒ
    const audio = new Audio(round.audio_url)
    audioRef.current = audio
    
    audio.onended = () => {
      startListening()
    }

    audio.onerror = (error) => {
      console.error('ì˜¤ë””ì˜¤ ì¬ìƒ ì‹¤íŒ¨:', error)
      alert('ì˜¤ë””ì˜¤ë¥¼ ì¬ìƒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
    }

    try {
      await audio.play()
    } catch (error) {
      console.error('ì˜¤ë””ì˜¤ ì¬ìƒ ì˜¤ë¥˜:', error)
    }
  }

  // ì‚¬ìš©ì ìŒì„± ê°ì§€ ë° ë…¹ìŒ ì‹œì‘
  const startListening = () => {
    console.log('ìŒì„± ì¸ì‹ ì‹œì‘')
    setPhase('listening')
    
    if (!mediaRecorderRef.current || !VoiceActivityDetector.isInitialized) {
      console.error('MediaRecorder ë˜ëŠ” VADê°€ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ')
      return
    }

    try {
      mediaRecorderRef.current.start(100)
      console.log('ë…¹ìŒ ì‹œì‘ë¨')
    } catch (error) {
      console.error('ë…¹ìŒ ì‹œì‘ ì‹¤íŒ¨:', error)
      return
    }
    
    // VAD (ìŒì„± ê°ì§€) ì¸í„°ë²Œ ì‹œì‘
    vadIntervalRef.current = setInterval(() => {
      const isVoice = VoiceActivityDetector.isVoiceDetected()
      
      if (isVoice) {
        // ìŒì„±ì´ ê°ì§€ë˜ë©´ ì¹¨ë¬µ íƒ€ì´ë¨¸ ë¦¬ì…‹
        if (silenceTimerRef.current) {
          clearTimeout(silenceTimerRef.current)
          silenceTimerRef.current = null
        }
      } else {
        // ì¹¨ë¬µì´ ê°ì§€ë˜ë©´ íƒ€ì´ë¨¸ ì‹œì‘ (ì´ë¯¸ ì‹œì‘ë˜ì§€ ì•Šì•˜ë‹¤ë©´)
        if (!silenceTimerRef.current) {
          silenceTimerRef.current = setTimeout(() => {
            console.log('ì¹¨ë¬µ ê°ì§€ë¨, ë…¹ìŒ ì¤‘ë‹¨')
            stopListening()
          }, VoiceActivityDetector.SILENCE_DURATION)
        }
      }
    }, 100)
  }

  // ë…¹ìŒ ì¤‘ë‹¨ ë° ì²˜ë¦¬
  const stopListening = () => {
    setPhase('processing')
    
    // VAD ê´€ë ¨ ì •ë¦¬
    if (vadIntervalRef.current) {
      clearInterval(vadIntervalRef.current)
      vadIntervalRef.current = null
    }
    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current)
      silenceTimerRef.current = null
    }

    // ë…¹ìŒ ì¤‘ë‹¨
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
      mediaRecorderRef.current.stop()
    }
  }

  // ë…¹ìŒ ì™„ë£Œ í›„ ì²˜ë¦¬ (STT ìš”ì²­)
  const handleRecordingComplete = async (audioBlob: Blob) => {
    try {
      setIsLoading(true)
      
      const formData = new FormData()
      formData.append('audio', audioBlob, 'recording.webm')
      formData.append('round', (currentRound + 1).toString())

      const response = await fetch('/api/stt', {
        method: 'POST',
        body: formData
      })

      if (!response.ok) {
        throw new Error(`STT ìš”ì²­ ì‹¤íŒ¨: ${response.status}`)
      }

      const sttResult = await response.json()
      
      const userResponse: UserResponse = {
        round: currentRound + 1,
        audioBlob,
        transcription: sttResult.transcription || ''
      }

      setUserResponses(prev => [...prev, userResponse])
      
      setCurrentRound(prev => prev + 1)
      
      setTimeout(() => {
        // ë‹¤ìŒ ë¼ìš´ë“œëŠ” useEffectì—ì„œ ìë™ìœ¼ë¡œ ì‹œì‘ë¨
      }, 1000)

    } catch (error) {
      console.error('ë…¹ìŒ ì²˜ë¦¬ ì‹¤íŒ¨:', error)
      alert('ìŒì„± ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.')
    } finally {
      setIsLoading(false)
    }
  }

  // ìµœì¢… ì‘ë‹µ ë¶„ì„ ë° ê²°ê³¼ í˜ì´ì§€ ì´ë™
  const analyzeResponses = async () => {
    try {
      setPhase('processing')
      setIsLoading(true)

      const analysisData = {
        scenario_id: scenario.id,
        scenario_title: scenario.title,
        questions: scenario.rounds.map(r => r.question),
        user_responses: userResponses.map(r => r.transcription || ''),
        guideline: scenario.guideline
      }

      const response = await fetch('/api/analyze', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify(analysisData)
      })

      if (!response.ok) {
        throw new Error(`ë¶„ì„ ìš”ì²­ ì‹¤íŒ¨: ${response.status}`)
      }

      const analysisResult = await response.json()
      
      sessionStorage.setItem('simulationResult', JSON.stringify({
        scenario,
        userResponses: userResponses.map(r => ({ 
          round: r.round, 
          transcription: r.transcription 
        })),
        analysis: analysisResult
      }))

      router.push('/simulation/results')

    } catch (error) {
      console.error('ë¶„ì„ ì‹¤íŒ¨:', error)
      alert('ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.')
    } finally {
      setIsLoading(false)
    }
  }

  // ëª¨ë“  ì˜¤ë””ì˜¤ ìì› ì •ë¦¬
  const cleanup = () => {
    console.log('ì˜¤ë””ì˜¤ ì •ë¦¬ ì‹œì‘')
    
    if (vadIntervalRef.current) {
      clearInterval(vadIntervalRef.current)
      vadIntervalRef.current = null
    }
    if (silenceTimerRef.current) {
      clearTimeout(silenceTimerRef.current)
      silenceTimerRef.current = null
    }
    if (audioRef.current) {
      audioRef.current.pause()
      audioRef.current = null
    }
    if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {
      mediaRecorderRef.current.stop()
    }
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => {
        track.stop()
        console.log('ì˜¤ë””ì˜¤ íŠ¸ë™ ì •ì§€:', track.kind)
      })
      streamRef.current = null
    }
    if (audioContextRef.current && audioContextRef.current.state !== 'closed') {
      audioContextRef.current.close()
      audioContextRef.current = null
    }
    
    VoiceActivityDetector.cleanup()
    console.log('ì˜¤ë””ì˜¤ ì •ë¦¬ ì™„ë£Œ')
  }

  // UI ìƒíƒœ ë©”ì‹œì§€
  const getPhaseMessage = () => {
    switch (phase) {
      case 'preparing':
        return 'ì‹œë®¬ë ˆì´ì…˜ì„ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤...'
      case 'playing':
        return `ë¼ìš´ë“œ ${currentRound + 1}: ìƒëŒ€ë°©ì´ ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤...`
      case 'listening':
        return 'ğŸ¤ ë‹¹ì‹ ì˜ ì‘ë‹µì„ ê¸°ë‹¤ë¦¬ê³  ìˆìŠµë‹ˆë‹¤. ë§ì”€í•´ì£¼ì„¸ìš”!'
      case 'processing':
        return 'ì‘ë‹µì„ ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤...'
      case 'completed':
        return 'ì‹œë®¬ë ˆì´ì…˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!'
      default:
        return ''
    }
  }

  // UI ìƒíƒœ ì•„ì´ì½˜
  const getPhaseIcon = () => {
    switch (phase) {
      case 'preparing':
        return 'âš™ï¸'
      case 'playing':
        return 'ğŸ“'
      case 'listening':
        return 'ğŸ¤'
      case 'processing':
        return 'â³'
      case 'completed':
        return 'âœ…'
      default:
        return ''
    }
  }

  // JSX ë°˜í™˜
  return (
    <div className="min-h-screen bg-black flex items-center justify-center p-4">
      <div className="max-w-2xl w-full">
        {/* í—¤ë” */}
        <div className="text-center mb-8">
          <h1 className="text-3xl font-bold text-white mb-2">ë³´ì´ìŠ¤í”¼ì‹± ì‹œë®¬ë ˆì´ì…˜</h1>
          <h2 className="text-xl text-gray-300 mb-4">{scenario.title}</h2>
          <div className="flex items-center justify-center space-x-2">
            <span className="text-4xl">{getPhaseIcon()}</span>
            <p className="text-lg text-gray-400">{getPhaseMessage()}</p>
          </div>
        </div>

        {/* ì§„í–‰ ìƒí™© */}
        <div className="bg-gray-900 border border-gray-700 rounded-lg p-6 mb-8">
          <div className="flex items-center justify-between mb-4">
            <span className="text-gray-400">ì§„í–‰ ìƒí™©</span>
            <span className="text-white">{currentRound} / {scenario.rounds.length}</span>
          </div>
          <div className="w-full bg-gray-700 rounded-full h-2">
            <div 
              className="bg-blue-600 h-2 rounded-full transition-all duration-500"
              style={{ width: `${(currentRound / scenario.rounds.length) * 100}%` }}
            />
          </div>
        </div>

        {/* í˜„ì¬ ë¼ìš´ë“œ ì •ë³´ */}
        {currentRound < scenario.rounds.length && (
          <div className="bg-gray-900 border border-gray-700 rounded-lg p-6 mb-8">
            <div className="flex items-center space-x-3 mb-4">
              <span className="bg-blue-600 text-white px-3 py-1 rounded-full text-sm font-medium">
                ë¼ìš´ë“œ {currentRound + 1}
              </span>
            </div>
            
            {phase === 'playing' && (
              <div className="text-gray-300">
                <p className="mb-2">ğŸ“ ìƒëŒ€ë°©:</p>
                <p className="text-lg italic border-l-4 border-red-500 pl-4">
                  &ldquo;{scenario.rounds[currentRound].question}&rdquo;
                </p>
              </div>
            )}

            {phase === 'listening' && (
              <div className="text-center">
                <div className="animate-pulse mb-4">
                  <div className="w-16 h-16 bg-red-600 rounded-full mx-auto flex items-center justify-center">
                    <span className="text-2xl">ğŸ¤</span>
                  </div>
                </div>
                <p className="text-white text-lg mb-2">ìŒì„±ì„ ê°ì§€í•˜ê³  ìˆìŠµë‹ˆë‹¤</p>
                <p className="text-gray-400 text-sm">ë§ì”€ì´ ëë‚˜ë©´ ìë™ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰ë©ë‹ˆë‹¤</p>
              </div>
            )}
          </div>
        )}

        {/* ë¡œë”© ìƒíƒœ */}
        {isLoading && (
          <div className="bg-gray-900 border border-gray-700 rounded-lg p-6 text-center">
            <div className="animate-spin rounded-full h-8 w-8 border-b-2 border-white mx-auto mb-4" />
            <p className="text-gray-400">ì²˜ë¦¬ ì¤‘...</p>
          </div>
        )}

        {/* ì™„ë£Œëœ ë¼ìš´ë“œë“¤ */}
        {userResponses.length > 0 && (
          <div className="bg-gray-900 border border-gray-700 rounded-lg p-6">
            <h3 className="text-lg font-semibold text-white mb-4">ì™„ë£Œëœ ì‘ë‹µ</h3>
            <div className="space-y-3">
              {userResponses.map((response, index) => (
                <div key={index} className="bg-gray-800 p-3 rounded-lg">
                  <div className="flex items-center space-x-2 mb-2">
                    <span className="bg-green-600 text-white px-2 py-1 rounded text-xs">
                      ë¼ìš´ë“œ {response.round}
                    </span>
                    <span className="text-green-400">âœ“</span>
                  </div>
                  <p className="text-gray-300 text-sm">
                    {response.transcription || 'ìŒì„± ë³€í™˜ ì¤‘...'}
                  </p>
                </div>
              ))}
            </div>
          </div>
        )}

        {/* ì•ˆë‚´ */}
        <div className="mt-8 text-center">
          <p className="text-gray-500 text-sm">
            ì´ ì‹œë®¬ë ˆì´ì…˜ì€ ë³´ì´ìŠ¤í”¼ì‹± ëŒ€ì‘ ëŠ¥ë ¥ í–¥ìƒì„ ìœ„í•œ ì—°ìŠµì…ë‹ˆë‹¤.
          </p>
        </div>
      </div>
    </div>
  )
}